---
title: "Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms"
collection: publications
category: conferences
permalink: /publication/conference_2025_neurips
excerpt: 'This paper presents a common task framework for evaluating scientific machine learning algorithms, focusing on their application to complex dynamical systems.'
date: 2025-12-01
venue: 'The 39th Annual Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track'
paperurl: 'https://openreview.net/forum?id=pkFKjmUE3L'
authors: P. M. Wyder, J. A. Goldfeder, A. Yermakov, Y. Zhao, Stefano Riva, J. P. Williams, D. Zoro, A. S. Rude, M. Tomasetto, J. Germany, J. Bakarji, G. Maierhofer, M. Cranmer, J. N. Kutz
---

*Authors*: P. M. Wyder, J. A. Goldfeder, A. Yermakov, Y. Zhao, Stefano Riva, J. P. Williams, D. Zoro, A. S. Rude, M. Tomasetto, J. Germany, J. Bakarji, G. Maierhofer, M. Cranmer, J. N. Kutz

[**Package Link**](https://github.com/CTF-for-Science/ctf4science)

*Abstract:* Machine learning (ML) is transforming modeling and control in the physical, engineering, and biological sciences. However, rapid development has outpaced the creation of standardized, objective benchmarksâ€”leading to weak baselines, reporting bias, and inconsistent evaluations across methods. This undermines reproducibility, misguides resource allocation, and obscures scientific progress. To address this, we propose a Common Task Framework (CTF) for scientific machine learning. The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. Inspired by the success of CTFs in fields like natural language processing and computer vision, our framework provides a structured, rigorous foundation for head-to-head evaluation of diverse algorithms. As a first step, we benchmark methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz. These results illustrate the utility of the CTF in revealing method strengths, limitations, and suitability for specific classes of problems and diverse objectives. Next, we are launching a competition around a global real world sea surface temperature dataset with a true holdout dataset to foster community engagement. Our long-term vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets that raise the bar for rigor and reproducibility in scientific ML.
